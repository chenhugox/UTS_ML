---
title: "Machine Learning: Mathematical Theory and Applications"
subtitle: "Computer lab 2"
author: 
  Group 12 
  - Hugo Xinghe CHEN
  & Jianhan WANG
date: last-modified
format: 
  html:
    self-contained: true
toc: true
execute:
  error: false
language: 
  title-block-author-single: " "
theme: Default
title-block-banner-color: Primary
editor: visual
---

```{=html}
<style>
.boxed-text {
  border: 2px solid black;
  padding: 10px;
  margin: 10px 0;
}
</style>
```
::: callout-warning
The instructions in this lab assume that the following packages are installed:

-   `splines`

-   `dplyr`

-   `caret`

-   `randomForest`

-   `xgboost`

-   `pROC`

-   `ggplot2`

Packages may be installed by executing `install.packages('packagename')`, where `'packagename'` is the name of the package, e.g. `'splines'`. You may have to install additional packages to solve the computer lab. If you want this file to run locally on your computer, you have to change some paths to where the data are stored (see below).
:::

## Introduction

This computer lab treats topics such as bagging, boosting, and learning parametric models by several optimisation algorithms.

------------------------------------------------------------------------

::: callout-note
### Instructions

In this computer lab, you will work in groups of two. The group hands in a single set of solutions. **It is strictly forbidden to copy codes or solutions from other groups, as well as the use of AI tools (such as ChatGPT)**. Not obeying these instructions is regarded as academic misconduct and may have serious consequences for you.

This computer lab is worth a total of 15 marks (the subject has a maximum of 100 marks). The maximum marks for a given problem is shown within parenthesis at the top of the section. The problems you should solve are marked with the symbol ðŸ’ª and surrounded by a box. Hand in the solutions and programming codes in the form of a html document generated by Quarto. **Before submitting, carefully check that the document compiles without any errors**. Use properly formatted figures and programming code.
:::

::: callout-warning
Not submitting according to the instructions may result in loss of marks. The same applies to poorly formatted reports (including poorly formatted code/figures).
:::

## 1. Bagging and boosting for bike rental data (regression) (2 marks)

Recall the `bike_rental_hourly.csv` dataset from Computer lab 1. The dataset can be downloaded from the Canvas page of the course[^1].

[^1]: The original data come from this [source](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset).

The dataset contains 17379 hourly observations over the two-year period January 1, 2011 - December 31, 2012. The dataset contains many features that may be useful for predicting the number of rides, such as the hour of the day, temperature, humidity, season, weekday, etc. In this section, we use the dataset that in addition contains features that capture the time series characteristics of the data. The following code builds that dataset, where we use the same training and test data as in Problem 4.2 in Computer lab 1.

```{r}
rm(list=ls()) # Remove variables 
cat("\014") # Clean workspace
suppressMessages(library(dplyr))
suppressMessages(library(splines))
bike_data <- read.csv('/Users/chenxinghe/Desktop/Hugo/ESILV/A5/S9_UTS/37401_ML/ComputerLab/2/bike_rental_hourly.csv')

head(bike_data)
bike_data$log_cnt <- log(bike_data$cnt)
bike_data$hour <- bike_data$hr/23 # transform [0, 23] to [0, 1]. 0 is midnight, 1 is 11 PM

# One hot for weathersit
one_hot_encode_weathersit <- model.matrix(~ as.factor(weathersit) - 1,data = bike_data)
one_hot_encode_weathersit  <- one_hot_encode_weathersit[, -1] # Remove reference category
colnames(one_hot_encode_weathersit) <- c('cloudy', 'light rain', 'heavy rain')
bike_data <- cbind(bike_data, one_hot_encode_weathersit)

# One hot for weekday
one_hot_encode_weekday <- model.matrix(~ as.factor(weekday) - 1,data = bike_data)
one_hot_encode_weekday  <- one_hot_encode_weekday[, -1] # Remove reference category
colnames(one_hot_encode_weekday) <- c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat')
bike_data <- cbind(bike_data, one_hot_encode_weekday)

# One hot for weekday
one_hot_encode_season <- model.matrix(~ as.factor(season) - 1,data = bike_data)
one_hot_encode_season  <- one_hot_encode_season[, -1] # Remove reference category
colnames(one_hot_encode_season) <- c('Spring', 'Summer', 'Fall')
bike_data <- cbind(bike_data, one_hot_encode_season)

# Create lags
bike_data_new <- mutate(bike_data, lag1 = lag(log_cnt, 1), lag2 = lag(log_cnt, 2), lag3 = lag(log_cnt, 3), lag4 = lag(log_cnt, 4), lag24 = lag(log_cnt, 24))

bike_data_new <- bike_data_new[-c(1:24),] # Lost 24 obs because of lagging

# Create training and test data
bike_all_data_train <- bike_data_new[bike_data_new$dteday >= as.Date("2011-01-01") & bike_data_new$dteday <=  as.Date("2012-05-31"), ]
bike_all_data_test <- bike_data_new[bike_data_new$dteday >= as.Date("2012-06-01") & bike_data_new$dteday <=  as.Date("2012-12-31"), ]
X_train <- cbind(1, bike_all_data_train[, c("lag1", "lag2",  "lag3", "lag4", "lag24")])
spline_basis <- ns(bike_all_data_train$hour, df = 10, intercept = FALSE)
X_train <- cbind(X_train, spline_basis)
colnames(X_train)[1] <- "intercept"
knots <- attr(spline_basis, "knots")
variables_to_keep_in_X <- c("yr", "holiday", "workingday", "temp", "atemp", "hum", "windspeed") 
variables_to_keep_in_X <- c(variables_to_keep_in_X, colnames(one_hot_encode_weathersit), colnames(one_hot_encode_weekday), colnames(one_hot_encode_season))
X_train <- cbind(X_train, bike_all_data_train[, variables_to_keep_in_X])
head(X_train)
# Training data
X_train <- as.matrix(X_train)
y_train <- bike_all_data_train$log_cnt
# Test data
y_test <- bike_all_data_test$log_cnt
X_test <- cbind(1, bike_all_data_test[, c("lag1", "lag2",  "lag3", "lag4", "lag24")])
spline_basis_test <- ns(bike_all_data_test$hour, df=10, knots=knots, intercept = FALSE)
X_test <- cbind(X_test, spline_basis_test)
colnames(X_test)[1] <- "intercept"
X_test <- cbind(X_test, bike_all_data_test[, variables_to_keep_in_X])
X_test <- as.matrix(X_test)
```

::: boxed-text
#### ðŸ’ª Problem 1.1

Use the `randomForest()` function in the `randomForest` package to fit three random forest regressions (bagging algorithms) with the `ntree` argument set to, respectively, 50, 100, and 500 (number of trees to grow). In each case, compute the root mean squared error (RMSE) for the training and test datasets.

::: callout-tip
The `predict()` function can be used on the object returned by `randomForest()`. The predict function has an argument `newdata`, which is used to specify the features to predict for.
:::
:::

```{r}
suppressMessages(library(randomForest))

rf_50 <- randomForest(X_train, y_train, ntree=50)
rf_100 <- randomForest(X_train, y_train, ntree=100)
rf_500 <- randomForest(X_train, y_train, ntree=500)

# Prediction
y_train_50 <- predict(rf_50, newdata = X_train)
y_test_50 <- predict(rf_50, newdata = X_test)

y_train_100 <- predict(rf_100, newdata = X_train)
y_test_100 <- predict(rf_100, newdata = X_test)

y_train_500 <- predict(rf_500, newdata = X_train)
y_test_500 <- predict(rf_500, newdata = X_test)

# Compute RMSE for training and test data
rmse_train_50 <- sqrt(sum((y_train - y_train_50)^2)/length(y_train))
rmse_test_50 <- sqrt(sum((y_test - y_test_50)^2)/length(y_test))

rmse_train_100 <- sqrt(sum((y_train - y_train_100)^2)/length(y_train))
rmse_test_100 <- sqrt(sum((y_test - y_test_100)^2)/length(y_test))

rmse_train_500 <- sqrt(sum((y_train - y_train_500)^2)/length(y_train))
rmse_test_500 <- sqrt(sum((y_test - y_test_500)^2)/length(y_test))

cat(" RMSE for ntree = 50:  Train:", rmse_train_50, ", Test:", rmse_test_50, "\n","RMSE for ntree = 100: Train:", rmse_train_100, ", Test:", rmse_test_100, "\n","RMSE for ntree = 500: Train:", rmse_train_500, " , Test:", rmse_test_500, "\n")
```

::: boxed-text
#### ðŸ’ª Problem 1.2

Plot a time series plot of the response in the original scale (i.e. counts and not log-counts) for the last week of the test data (last $24\times 7$ observations). In the same figure, plot a time series plot of the fitted values (in the original scale) from Problem 1.1 when using `ntree=50` and `ntree=100.` Comment on the results.
:::

```{r}
# Last week of test data and 168=24*7
last_week_test <- (nrow(bike_all_data_test) - 168 +1):nrow(bike_all_data_test)
y_test_last_week <- bike_all_data_test$log_cnt[last_week_test]
y_test_original <- exp(y_test_last_week)

y_test_50_last_week <- predict(rf_50, newdata = X_test[last_week_test, ])
y_test_100_last_week <- predict(rf_100, newdata = X_test[last_week_test, ])

# Transform into original scale
y_test_50_original <- exp(y_test_50_last_week)
y_test_100_original <- exp(y_test_100_last_week)

plot(1:168, y_test_original, type = 'l', col = 'black', lwd = 2, ylim = range(c(y_test_original, y_test_50_original, y_test_100_original)), xlab = 'Hour', ylab = 'Bike Rentals (Counts)', main = 'Bike Rentals Prediction- Last Week')

lines(1:168, y_test_50_original, col = 'blue', lwd = 2, lty = 3)  
lines(1:168, y_test_100_original, col = 'red', lwd = 2, lty = 3)  
legend("topleft", legend=c("Actual", "ntree=50", "ntree=100"), 
       col=c("black", "blue", "red"), lwd=2, lty=c(1, 3, 3))
```

```{=html}
We can see here that both predictions seem to be comparatively accurate. And the prediction of "ntree=100" is a little bit smoother than the one of "ntree=50". This correspond the RMSE results in Problem 1.1. So, if the number of trees to grow goes higher, then this random forest model predicts more accurately.
```
::: boxed-text
#### ðŸ’ª Problem 1.3

Use the `xgboost()` function in the `xgboost` package to fit three extreme gradient boosting regressions (boosting algorithms) with the `nrounds` argument set to, respectively, 10, 25, and 50 (max number of boosting iterations). In each case, compute the root mean squared error (RMSE) for the training and test datasets.

::: callout-tip
The `xgboost()` function has an argument `data` to provide the features, and an argument `label` to provide the response. The `predict()` function can be used on the object returned by `xgboost()`. The predict function has an argument `newdata`, which is used to specify the features to predict for.
:::
:::

```{r}
suppressMessages(library(xgboost))

xgb_model_10 <- xgboost(data = X_train, label = y_train, nrounds = 10, verbose = 0)
xgb_model_25 <- xgboost(data = X_train, label = y_train, nrounds = 25, verbose = 0)
xgb_model_50 <- xgboost(data = X_train, label = y_train, nrounds = 50, verbose = 0)

y_train_10 <- predict(xgb_model_10, newdata = X_train)
y_test_10 <- predict(xgb_model_10, newdata = X_test)

y_train_25 <- predict(xgb_model_25, newdata = X_train)
y_test_25 <- predict(xgb_model_25, newdata = X_test)

y_train_50 <- predict(xgb_model_50, newdata = X_train)
y_test_50 <- predict(xgb_model_50, newdata = X_test)

# Compute RMSE for training and test data
rmse_train_10 <- sqrt(sum((y_train - y_train_10)^2)/length(y_train))
rmse_test_10 <- sqrt(sum((y_test - y_test_10)^2)/length(y_test))

rmse_train_25 <- sqrt(sum((y_train - y_train_25)^2)/length(y_train))
rmse_test_25 <- sqrt(sum((y_test - y_test_25)^2)/length(y_test))

rmse_train_50 <- sqrt(sum((y_train - y_train_50)^2)/length(y_train))
rmse_test_50 <- sqrt(sum((y_test - y_test_50)^2)/length(y_test))

cat(" RMSE for nrounds = 10: Train:", rmse_train_10, ", Test:", rmse_test_10, "\n","RMSE for nrounds = 25: Train:", rmse_train_25, ", Test:", rmse_test_25, "\n","RMSE for nrounds = 50: Train:", rmse_train_50, ", Test:", rmse_test_50, "\n")
```

::: boxed-text
#### ðŸ’ª Problem 1.4

Plot the response in the original scale for the last week of the test data (last $24\times 7$ observations) vs the corresponding fitted values from the best bagging model in Problem 1.2 and the best boosting model in Problem 1.3. Comment on the results.
:::

```{r}
last_week_test <- (nrow(bike_all_data_test) - 168 +1):nrow(bike_all_data_test)

# Best of 1.2: rf_100 and best of 1.3: xgb_model_50
y_test_original <- exp(y_test_last_week)
y_test_pred_bagging <- predict(rf_100, newdata = X_test[last_week_test, ])
y_test_pred_boosting <- predict(xgb_model_50, newdata = X_test[last_week_test, ])

# Transform into original scale
y_test_pred_bagging_original <- exp(y_test_pred_bagging)
y_test_pred_boosting_original <- exp(y_test_pred_boosting)

plot(1:168, y_test_original, type = 'l', col = 'black', lwd = 2, ylim = range(c(y_test_original, y_test_pred_bagging_original, y_test_pred_boosting_original)), xlab = 'Hour', ylab = 'Bike Rentals (Counts)', main = 'Bike Rentals Prediction- Last Week')

lines(1:168, y_test_pred_bagging_original, col = 'blue', lwd = 2, lty = 3)
lines(1:168, y_test_pred_boosting_original, col = 'red', lwd = 2, lty = 3) 
legend("topleft", legend=c("Actual", "Bagging (ntree=100)", "Boosting (nrounds=50)"), col=c("black", "blue", "red"), lwd=2, lty=c(1, 3, 3))
```

```{=html}
We can see here that the prediction of "ntree=100" bagging model globally predicts better to actual data than the one of "nrounds=50" boosting model. This also correspond to their RMSE results which were calculated in Problem 1.1 and 1.3.
```
## 2. Bagging and boosting for spam email data (classification) (2 marks)

Recall the `spam_ham_emails.RData` dataset from Computer lab 1. The dataset consists of $n=4601$ spam ($y=1$) and ham (no spam, $y=0$) emails with corresponding 15 features and can be downloaded from the Canvas page of the course[^2].

[^2]: The original data come from this [source](https://archive.ics.uci.edu/ml/datasets/spambase). The dataset used here includes a subset of the 57 original features.

Most of the features are continuous real variables in the interval $[0, 100]$, with values corresponding to the percentage of occurrence of a specific word or character in the email. There are also a few features that capture the tendency to use many capital letters. The following code defines the response variable as a factor, standardises the features, and creates a partition with 75% of the data for training and 25% for testing.

```{r}
rm(list=ls()) # Remove variables 
cat("\014") # Clean workspace
load(file = '/Users/chenxinghe/Desktop/Hugo/ESILV/A5/S9_UTS/37401_ML/ComputerLab/2/spam_ham_emails.RData')

Spam_ham_emails[, -1] <- scale(Spam_ham_emails[, -1])
Spam_ham_emails['spam'] <- as.factor(Spam_ham_emails['spam'] == 1) # Changing from 1->TRUE, 0->FALSE

levels(Spam_ham_emails$spam) <- c("not spam", "spam")
head(Spam_ham_emails)
set.seed(1234)
suppressMessages(library(caret))

train_obs <- createDataPartition(y = Spam_ham_emails$spam, p = .75, list = FALSE)
train <- Spam_ham_emails[train_obs, ]
y_train <- train$spam
X_train <- train[, -1]
test <- Spam_ham_emails[-train_obs, ]
y_test <- test$spam
X_test <- test[, -1]

# Confirm both training and test are balanced with respect to spam emails
cat("Percentage of training data consisting of spam emails:", 100*mean(train$spam == "spam"),"\n")
cat("Percentage of test data consisting of spam emails:", 100*mean(test$spam == "spam"),"\n")
```

::: boxed-text
#### ðŸ’ª Problem 2.1

Use the `randomForest()` function in the `randomForest` package to fit three random forest classification with the `ntree` argument set to, respectively, 50, 100, and 500. In each case, plot the ROC curve and compute the area under the ROC curve (AUC) for the test data using the package `pROC()` (all plots in the same figure). Comment on the results.

::: callout-tip
In classification, the `predict()` function provides a sharp prediction when used on the object returned by `randomForest()`. To predict a probability instead, the argument `type="prob"` to the `predict()` function can be specified.
:::
:::

```{r}
suppressMessages(library(randomForest))
suppressMessages(library(pROC))
suppressMessages(library(ggplot2))

rf_50 <- randomForest(x = X_train, y = y_train, ntree = 50)
rf_100 <- randomForest(x = X_train, y = y_train, ntree = 100)
rf_500 <- randomForest(x = X_train, y = y_train, ntree = 500)

# Predict probabilities on test data for each model
prob_rf_50 <- predict(rf_50, newdata = X_test, type = "prob")[, 2]
prob_rf_100 <- predict(rf_100, newdata = X_test, type = "prob")[, 2]
prob_rf_500 <- predict(rf_500, newdata = X_test, type = "prob")[, 2]

# Compute ROC curves
roc_rf_50 <- roc(y_test, prob_rf_50, levels = rev(levels(y_test)), plot = FALSE, direction = ">")
roc_rf_100 <- roc(y_test, prob_rf_100, levels = rev(levels(y_test)), plot = FALSE, direction = ">")
roc_rf_500 <- roc(y_test, prob_rf_500, levels = rev(levels(y_test)), plot = FALSE, direction = ">")

plot(roc_rf_50, col = "blue", main = "ROC Curves for Random Forest Models", lwd = 2)
plot(roc_rf_100, col = "green", add = TRUE, lwd = 2)
plot(roc_rf_500, col = "red", add = TRUE, lwd = 2)
legend("bottomright", legend = c("ntree = 50", "ntree = 100", "ntree = 500"), col = c("blue", "green", "red"), lwd = 2)

# Calculate AUC
auc_rf_50 <- auc(roc_rf_50)
auc_rf_100 <- auc(roc_rf_100)
auc_rf_500 <- auc(roc_rf_500)

cat(" AUC for ntree = 50: ", auc_rf_50, "\n",
    "AUC for ntree = 100:", auc_rf_100, "\n",
    "AUC for ntree = 500:", auc_rf_500, "\n")
```

```{=html}
The results show that AUC of "ntree=500" is the biggest value among those of "ntree=50" and "ntree=100". We can say that higher the number of tree is, better the model performs in classification.
```
::: boxed-text
#### ðŸ’ª Problem 2.2

Predict the test data using the random forest classifier in Problem 2.1 with `ntree=100` following the rule: if $\mathrm{Pr}(y=1|\mathbf{x})>0.5 \Rightarrow y=1$, and compute the confusion matrix using the `confusionMatrix()` function from the `caret` package.
:::

```{r}
suppressMessages(library(caret))

rf_100 <- randomForest(x = X_train, y = y_train, ntree = 100)

prob_rf_100 <- predict(rf_100, newdata = X_test, type = "prob")[, 2]

# Apply the rule
y_pred_rf_100 <- ifelse(prob_rf_100 > 0.5, "spam", "not spam")
y_pred_rf_100 <- factor(y_pred_rf_100, levels = levels(y_test))

conf_matrix <- confusionMatrix(y_pred_rf_100, y_test)

print(conf_matrix)
```

We now turn our attention to boosting performed by the `xgboost()` function in the `xgboost` package. This function does not accept data frames as input (`X_train` above, among others, is of this type). Moreover, the function does not accept variables that are coded as factors. For logistic regression, it requires that the response is coded 0/1, where 1 is success. The following code transforms the features and response variable to this format.

```{r}
X_train_xgb <- as.matrix(X_train)
X_test_xgb <- as.matrix(X_test)
y_train_xgb <- as.integer(y_train) - 1
y_test_xgb <- as.integer(y_test) - 1
```

::: boxed-text
#### ðŸ’ª Problem 2.3

Use the `xgboost()` function in the `xgboost` package to fit three extreme gradient boosting regressions with the `nrounds` argument set to, respectively, 10, 25, and 50. In each case, plot the ROC curve and compute the area under the ROC curve (AUC) for the test data using the package `pROC()` (all plots in the same figure). Moreover, add the ROC plot for the random forest in Problem 2.1 with `ntree=100`. Comment on the results.

::: callout-tip
To get the `xgboost()` function to perform classification (instead of regression), supply the argument `objective="binary:logistic"`.
:::
:::

```{r}
suppressMessages(library(xgboost))

rf_100 <- randomForest(x = X_train, y = y_train, ntree = 100)
prob_rf_100 <- predict(rf_100, newdata = X_test, type = "prob")[, 2]
roc_rf_100 <- roc(y_test, prob_rf_100, levels = rev(levels(y_test)), plot = FALSE, direction = ">")

params <- list(objective = "binary:logistic", eval_metric = "auc")

xgb_model_10 <- xgboost(data = X_train_xgb, label = y_train_xgb, nrounds = 10, verbose = 0, objective="binary:logistic")
xgb_model_25 <- xgboost(data = X_train_xgb, label = y_train_xgb, nrounds = 25, verbose = 0, objective="binary:logistic")
xgb_model_50 <- xgboost(data = X_train_xgb, label = y_train_xgb, nrounds = 50, verbose = 0, objective="binary:logistic")

prob_xgb_10 <- predict(xgb_model_10, newdata = X_test_xgb)
prob_xgb_25 <- predict(xgb_model_25, newdata = X_test_xgb)
prob_xgb_50 <- predict(xgb_model_50, newdata = X_test_xgb)

roc_xgb_10 <- roc(y_test_xgb, prob_xgb_10, plot = FALSE, direction = "<")
roc_xgb_25 <- roc(y_test_xgb, prob_xgb_25, plot = FALSE, direction = "<")
roc_xgb_50 <- roc(y_test_xgb, prob_xgb_50, plot = FALSE, direction = "<")

plot(roc_xgb_10, col = "blue", main = "ROC Curves for XGBoost Models and Random Forest", lwd = 2)
plot(roc_xgb_25, col = "green", add = TRUE, lwd = 2)
plot(roc_xgb_50, col = "red", add = TRUE, lwd = 2)

plot(roc_rf_100, col = "black", add = TRUE, lwd = 2)

legend("bottomright", legend = c("XGBoost (nrounds = 10)", "XGBoost (nrounds = 25)", "XGBoost (nrounds = 50)", "Random Forest (ntree = 100)"), col = c("blue", "green", "red", "black"), lwd = 2)

# Calculate AUC
auc_xgb_10 <- auc(roc_xgb_10)
auc_xgb_25 <- auc(roc_xgb_25)
auc_xgb_50 <- auc(roc_xgb_50)
auc_rf_100 <- auc(roc_rf_100)

cat(" AUC for XGBoost (nrounds = 10):     ", auc_xgb_10, "\n",
    "AUC for XGBoost (nrounds = 25):     ", auc_xgb_25, "\n",
    "AUC for XGBoost (nrounds = 50):     ", auc_xgb_50, "\n",
    "AUC for Random Forest (ntree = 100):", auc_rf_100, "\n")
```

```{=html}
In case of boosting regression on classification, we can see that "nrounds=25" gets the best result. And the random forest model performs the worst with "ntree=100".
```
::: boxed-text
#### ðŸ’ª Problem 2.4

Predict the test data using the extreme gradient boosting classifier in Problem 2.3 with `nrounds=25` following the rule: if $\mathrm{Pr}(y=1|\mathbf{x})>0.5 \Rightarrow y=1$, and compute the confusion matrix using the `confusionMatrix()` function from the `caret` package.
:::

```{r}
xgb_model_25 <- xgboost(data = X_train_xgb, label = y_train_xgb, nrounds = 25, verbose = 0, objective="binary:logistic")

prob_xgb_25 <- predict(xgb_model_25, newdata = X_test_xgb)
y_pred_xgb_25 <- ifelse(prob_xgb_25 > 0.5, 1, 0)  

# Convert to factors
y_pred_xgb_25 <- factor(y_pred_xgb_25, levels = c(0, 1), labels = c("not spam", "spam"))
y_test_xgb_factor <- factor(y_test_xgb, levels = c(0, 1), labels = c("not spam", "spam"))

conf_matrix_xgb_25 <- confusionMatrix(y_pred_xgb_25, y_test_xgb_factor)

print(conf_matrix_xgb_25)
```

## 3. Learning parametric models by gradient based optimisation (4 marks)

In this problem, we will learn how to implement gradient based optimisers (non-stochastic as well as stochastic) to learn the parameters of a parametric statistical model. When implementing a learning algorithm, it is advisable to use simulated data first to ensure that the algorithm works properly. With simulated data we know the answer, i.e. the true model parameters, and can thus check if our algorithm manages to learn (estimate) them correctly. Due to the randomness of the data generating process, however, we will not be able to obtain the exact values. We hope for estimates that are reasonably close[^3].

[^3]: Although not pursued here, we can also construct asymptotic confidence intervals using the asymptotic properties of maximum likelihood estimators. Alternatively, a Bayesian approach gives a posterior probability distribution for the unknown quantity, and one can check if the true model parameters are within a credible set. The advantage of the latter is that it does not rely on asymptotics.

Our running example will be a Poisson regression model, where the response follows the Poisson distribution, i.e.

$$y|\beta, \mathbf{x} \sim \mathrm{Poisson}(\exp\left(\mathbf{x}^\top \mathbf{\beta}\right)).$$ {#eq-PoisReg}

To keep notation simple, we sometimes skip conditioning on $\mathbf{x}$ since we assume they are known (we only model $y$). To proceed simulating, say $n=1000$, $y$ observations, we need given values of $x$ first. The following function generates $n=1000$ such observations uniformly on the unit interval[^4]. It uses the `set.seed()` function to ensure the same features are generated every time.

[^4]: Unlike when we generate y, we can choose any distribution for x because it is not modelled.

```{r}
rm(list=ls()) # Remove variables 
cat("\014") # Clean workspace
gen_x <- function(seed=123){
  set.seed(seed)
  return(runif(0, 1, n = 1000))
}
# Test
x <- gen_x()
length(x)
head(x)
```

::: boxed-text
#### ðŸ’ª Problem 3.1

Simulate $n=1000$ independent (conditionally on $x$) $y$ observations from the Poisson regression model $$y|\beta_0,\beta_1 \sim \mathrm{Poisson}(\exp\left(\beta_0 + \beta_1x \right)),$$ with the true parameters $\beta_0= 0.3$ and $\beta_1= 2.5$.

::: callout-tip
The `rpois()` function can be used to simulate random numbers from the Poisson distribution. The argument `lambda` is used to specify the expected value of the distribution.
:::
:::

```{r}
beta_0 <- 0.3
beta_1 <- 2.5

# Simulate y
y <- rpois(1000, lambda = exp(beta_0 + beta_1 * x))

head(x)
head(y)
```

::: boxed-text
#### ðŸ’ª Problem 3.2

In the same figure, plot the scatter $\{x_i, y_i\}_{i=1}^n$ and the conditional expected value $\mathbb{E}(y|x)$ as a function of $x$ given the true parameter values.

::: callout-tip
When plotting the conditional expected value as a function of $x$, create a variable `x_grid<-seq(0, 1, length.out=10000)` and evaluate the expected value on the grid.
:::
:::

```{r}
x_grid <- seq(0, 1, length.out = 10000)

expected_value <- exp(beta_0 + beta_1 * x_grid)

plot(x, y, main = "Scatter xy and Conditional Expected Value E(y|x)", xlab = "x", ylab = "y", pch = 20, col = "blue")

lines(x_grid, expected_value, col = "red", lwd = 2)
legend("topright", legend = c("Scatter (x, y)", "E(y|x)"),
       col = c("blue", "red"), pch = c(20, NA), lty = c(NA, 1), lwd = 2)
```

We now turn our attention to learning (estimating) the parameters $\beta_0$ and $\beta_1$ from the dataset. The cross-entropy between the empirical distribution of the data and the parametric model is minimised when the log-likelihood is used as a loss function (cost function) for learning the parameters. The log-likelihood is defined as the log of the density (to be exact, the probability mass function here since $y$ is discrete) of the sample **viewed as a function of the parameters (with the sample fixed)**, which for our model is $$\ell(\beta_0, \beta_1)=\log p(y_1,y_2,\dots,y_n|x_1,x_2,\dots,x_n,\beta_0, \beta_1).$$

::: boxed-text
#### ðŸ’ª Problem 3.3

Derive (analytically) the log-likelihood $\ell(\beta_0, \beta_1)$ for the Poisson regression model.

::: callout-tip
The probability mass function of the Poisson model with expected value $\mu>0$ is given by $$p(y|\mu)=\frac{\mu^y\exp(-\mu)}{y!}.$$ Now use that the $y$s are independent (conditionally on the $x$s) and think about how $\mu$ relates to $\beta_0, \beta_1$ in our model. Finally, do not forget about the log!
:::
:::

```{r}
mu <- exp(beta_0 + beta_1 * x)
log_likelihood <- sum(y * log(mu) - mu - lfactorial(y)) 

log_likelihood
```

Define the cost function $J(\beta_0, \beta_1) = -\frac{1}{n}\ell(\beta_0, \beta_1)$, i.e. as the average negative log-density of the observations (viewed as a function of the parameters to learn). The motivation of the form of the cost function is the following. First, note that the larger the log-likelihood for a set of parameter values is, the more plausible that those parameter values generated the given dataset. Thus, to turn the log-likelihood to a cost -- a quantity where less (and not more) is better -- it is multiplied by a negative sign. The average part of the cost function (division by $n$) is so that it is comparable over datasets of different sizes.

The cost function $J(\beta_0, \beta_1)$ is the objective function we minimise to learn the parameters, i.e. our estimates $\widehat{\beta}=(\widehat{\beta}_0,\widehat{\beta}_1)$ are obtained as $$\widehat{\beta}=\arg \min_{\beta_0, \beta_1} J(\beta_0, \beta_1).$$

Your task is to implement a gradient descent algorithm to minimise $J(\beta_0, \beta_1)$ for the Poisson regression model. This requires two functions, one that evaluates $J(\beta_0, \beta_1) = -\frac{1}{n}\ell(\beta_0, \beta_1)$, and another that evaluates its gradient, i.e. $\nabla J(\beta_0, \beta_1) = -\frac{1}{n}\nabla\ell(\beta_0, \beta_1)$.

::: boxed-text
#### ðŸ’ª Problem 3.4

Derive (analytically) the gradient of the log-likelihood $\ell(\beta_0, \beta_1)$ for the Poisson regression model.

::: callout-tip
Because of the (conditional) independence assumption $$\nabla \ell(\beta_0, \beta_1)=\sum_{i=1}^n \nabla \log p(y_i|x_i,\beta_0, \beta_1),$$ and note that $$\nabla \log p(y_i|x_i,\beta_0, \beta_1)=\left(\frac{\partial}{\partial\beta_0}\log p(y_i|x_i,\beta_0, \beta_1), \frac{\partial}{\partial\beta_1}\log p(y_i|x_i,\beta_0, \beta_1) \right)^\top.$$
:::
:::

```{r}
gradient_log_likelihood <- function(beta_0, beta_1, x, y) {
  mu <- exp(beta_0 + beta_1 * x)
  
  grad_beta_0 <- sum(y - mu)
  grad_beta_1 <- sum((y - mu) * x)
  
  return(c(grad_beta_0, grad_beta_1))
}

gradient_values <- gradient_log_likelihood(beta_0, beta_1, x, y)

gradient_values
```

::: boxed-text
#### ðŸ’ª Problem 3.5

Code two functions, one that evaluates $\ell(\beta_0, \beta_1)$, and another that evaluates its gradient, i.e. $\nabla\ell(\beta_0, \beta_1)$. Note that the first function is scalar valued, whereas the second is vector valued ($2\times 1$ in our case).
:::

```{r}
x <- gen_x()

beta_0 <- 0.3
beta_1 <- 2.5

y <- rpois(1000, lambda = exp(beta_0 + beta_1 * x)) 

# Function for the log-likelihood
log_likelihood <- function(beta_0, beta_1, x, y) {
  mu <- exp(beta_0 + beta_1 * x)
  log_likelihood_value <- sum(y * log(mu) - mu - lfactorial(y)) 
  return(log_likelihood_value)
}

# Function for the gradient
gradient_log_likelihood <- function(beta_0, beta_1, x, y) {
  mu <- exp(beta_0 + beta_1 * x)
  
  grad_beta_0 <- sum(y - mu)
  grad_beta_1 <- sum((y - mu) * x)
  
  return(c(grad_beta_0, grad_beta_1))
}

log_likelihood_value <- log_likelihood(beta_0, beta_1, x, y)
gradient_values <- gradient_log_likelihood(beta_0, beta_1, x, y)

cat(" Log-Likelihood: ", log_likelihood_value, "\n",
    "Gradient: ",       gradient_values, "\n")
```

::: boxed-text
#### ðŸ’ª Problem 3.6

Implement gradient descent (Algorithm 5.1 in Lindholm et al. (2022)[^5]) to learn the parameters in the Poisson regression model by minimising the cost function $J(\beta_0, \beta_1)$. Implement three different algorithms, with step sizes $\gamma=0.01, 0.1, 0.25$, and using $(0,0)$ as starting value. Note that Algorithm 5.1 has a stopping condition which states that it stops when the change in the parameter updates from one iteration to the other is small enough. If a wrong implementation is given, or the step size is chosen such that the algorithm does not converge, the code might get stuck in an infinite while loop (the condition to exit the loop is never met). For this reason, we here perform 200 iterations of the algorithm only (regardless of convergence or not). Comment on the results.

::: callout-tip
Code two functions, one that evaluates $J(\beta_0, \beta_1) = -\frac{1}{n}\ell(\beta_0, \beta_1)$ (scalar valued function), and another that evaluates its gradient, i.e. $\nabla J(\beta_0, \beta_1) = -\frac{1}{n}\nabla\ell(\beta_0, \beta_1)$ (vector valued function). Call these two functions when implementing the gradient descent algorithm.

To inspect the effects of different learning rates one may plot (for example):

-   $J(\beta_0^{(t)}, \beta_1^{(t)})$, where $t$ is the iteration number and $\beta_0^{(t)}, \beta_1^{(t)}$ are the corresponding updates at that iteration. Plotting this quantity vs the iteration number illustrates the convergence as it should settle when reaching the minimum. The faster it settles, the faster the convergence.

-   $||\nabla J(\beta_0^{(t)}, \beta_1^{(t)})||_2$ , i.e. the Euclidean norm of the gradient, at each iterate as a function of the iteration number. This should eventually go to zero (the gradient is zero at the optimal value). The Euclidean norm of a vector `v` can be computed by `norm(v, type="2")`.

-   $\beta_0^{(t)}, \beta_1^{(t)}$, i.e. the updates vs the iteration number. These should converge to (the vicinity of) the true values.
:::
:::

[^5]: Lindholm, A., WahlstrÃ¶m, N., Lindsten, F. and SchÃ¶n, T. B (2022). Machine Learning - A First Course for Engineers and Scientists. Cambridge University Press.

```{r}
# Function to calculate the cost function J
cost_function <- function(beta_0, beta_1, x, y) {
  n <- length(y)
  log_likelihood_value <- log_likelihood(beta_0, beta_1, x, y)
  return(-log_likelihood_value / n)
}

# Function to calculate the gradient of J
gradient_cost_function <- function(beta_0, beta_1, x, y) {
  n <- length(y)
  grad <- gradient_log_likelihood(beta_0, beta_1, x, y)
  return(-grad / n)
}

# Gradient descent algorithm
gradient_descent <- function(x, y, beta_0, beta_1, step_size, max_iter = 200) {
  beta_0_history <- numeric(max_iter)
  beta_1_history <- numeric(max_iter)
  cost_history <- numeric(max_iter)
  norm_gradient_history <- numeric(max_iter)
  
  for (i in 1:max_iter) {
    current_cost <- cost_function(beta_0, beta_1, x, y)
    current_gradient <- gradient_cost_function(beta_0, beta_1, x, y)
    
    beta_0_history[i] <- beta_0
    beta_1_history[i] <- beta_1
    cost_history[i] <- current_cost
    norm_gradient_history[i] <- norm(current_gradient, type = "2")
    
    beta_0 <- beta_0 - step_size * current_gradient[1]
    beta_1 <- beta_1 - step_size * current_gradient[2]
  }
  
  return(list(beta_0_history = beta_0_history, 
              beta_1_history = beta_1_history, 
              cost_history = cost_history,
              norm_gradient_history = norm_gradient_history))
}

#Example data
x <- gen_x()
y <- rpois(1000, lambda = exp(0.3 + 2.5 * x))  

beta_0_init <- 0
beta_1_init <- 0

# Perform gradient descent
step_sizes <- c(0.01, 0.1, 0.25)
results <- lapply(step_sizes, function(step_size) {
  gradient_descent(x, y, beta_0_init, beta_1_init, step_size)
})

par(mfrow = c(2, 2))


for (i in 1:length(step_sizes)) {
  plot(results[[i]]$cost_history, type = "l", col = i, 
       main = paste("Cost Function with Step Size =", step_sizes[i]), 
       xlab = "Iteration", ylab = "Cost")
}

par(mfrow = c(2, 2))


for (i in 1:length(step_sizes)) {
  plot(results[[i]]$norm_gradient_history, type = "l", col = i, 
       main = paste("Norm of Gradient with Step Size =", step_sizes[i]), 
       xlab = "Iteration", ylab = "Gradient Norm")
}

par(mfrow = c(2, 2))


for (i in 1:length(step_sizes)) {
  plot(results[[i]]$beta_0_history, type = "l", col = i, 
       main = paste("Beta_0 with Step Size =", step_sizes[i]), 
       xlab = "Iteration", ylab = "Beta_0")
}

par(mfrow = c(2, 2))


for (i in 1:length(step_sizes)) {
  plot(results[[i]]$beta_1_history, type = "l", col = i, 
       main = paste("Beta_1 with Step Size =", step_sizes[i]), 
       xlab = "Iteration", ylab = "Beta_1")
}

par(mfrow = c(1, 1)) 
```

```{=html}
In the cost function, the algorithm converged when the step sizes = 0.01 and 0.1, and it was faster when step size = 0.1. For the step sizes = 0.25, the algorithm converged at the beginning but showed divergence afterwards.

While the gradient function, Beta_0, and Beta_1 all showed the same phenomenon as the cost function did.
```
::: boxed-text
#### ðŸ’ª Problem 3.7

Run stochastic gradient descent (Algorithm 5.3 in Lindholm et al. (2022)) for $E=20$ epochs to learn the parameters in the Poisson regression model. Experiment with three different mini-batch sizes, $n_b=10,50,100$ and use the diminishing learning rate $\gamma^{(t)}=0.5/t^{0.6}$. Which mini-batch size seems to converge the fastest?

::: callout-tip
Some of the suggested figures in Problem 3.6 may be useful for illustrating the effects of different mini-batch sizes.
:::
:::

```{r}
# Stochastic Gradient Descent algorithm
sgd_poisson <- function(x, y, beta_0, beta_1, mini_batch_size, epochs = 20) {
  n <- length(y)
  beta_0_history <- numeric(epochs * (n %/% mini_batch_size))
  beta_1_history <- numeric(epochs * (n %/% mini_batch_size))
  cost_history <- numeric(epochs * (n %/% mini_batch_size))
  gradient_history <- numeric(epochs * (n %/% mini_batch_size))
  
  k <- 1 
  for (epoch in 1:epochs) {
    idx <- sample(1:n)
    x <- x[idx]
    y <- y[idx]
    
    # Mini-batch
    for (i in seq(1, n, by = mini_batch_size)) {
      if ((i + mini_batch_size - 1) > n) break
      x_batch <- x[i:(i + mini_batch_size - 1)]
      y_batch <- y[i:(i + mini_batch_size - 1)]
      
      step_size <- 0.5 / (k^0.6)
      
      grad <- gradient_cost_function(beta_0, beta_1, x_batch, y_batch)
      
      beta_0 <- beta_0 - step_size * grad[1]
      beta_1 <- beta_1 - step_size * grad[2]
      
      beta_0_history[k] <- beta_0
      beta_1_history[k] <- beta_1
      cost_history[k] <- cost_function(beta_0, beta_1, x, y)
      gradient_history[k] <- norm(grad, type = "2")
      
      k <- k + 1
    }
  }
  
  return(list(beta_0_history = beta_0_history, 
              beta_1_history = beta_1_history, 
              cost_history = cost_history,
              gradient_history = gradient_history))
}

# Example data
x <- gen_x()
y <- rpois(1000, lambda = exp(0.3 + 2.5 * x))

beta_0_init <- 0
beta_1_init <- 0

# Perform SGD
mini_batch_sizes <- c(10, 50, 100)
results_sgd <- lapply(mini_batch_sizes, function(mini_batch_size) {
  sgd_poisson(x, y, beta_0_init, beta_1_init, mini_batch_size)
})


par(mfrow = c(2, 2))

for (i in 1:length(mini_batch_sizes)) {
  plot(results_sgd[[i]]$cost_history, type = "l", col = i, 
       main = paste("Cost with Mini-batch Size =", mini_batch_sizes[i]), 
       xlab = "Iteration", ylab = "Cost")
}

par(mfrow = c(2, 2))

for (i in 1:length(mini_batch_sizes)) {
  plot(results_sgd[[i]]$gradient_history, type = "l", col = i, 
       main = paste("Gradient with Mini-batch Size =", mini_batch_sizes[i]), 
       xlab = "Iteration", ylab = "Gradient")
}

par(mfrow = c(2, 2))

for (i in 1:length(mini_batch_sizes)) {
  plot(results_sgd[[i]]$beta_0_history, type = "l", col = i, 
       main = paste("Beta_0 with Mini-batch Size =", mini_batch_sizes[i]), 
       xlab = "Iteration", ylab = "Beta_0")
}

par(mfrow = c(2, 2))

for (i in 1:length(mini_batch_sizes)) {
  plot(results_sgd[[i]]$beta_1_history, type = "l", col = i, 
       main = paste("SGD Beta_1 with Mini-batch Size =", mini_batch_sizes[i]), 
       xlab = "Iteration", ylab = "Beta_1")
}
```

```{=html}
All three mini-batch sizes converged in the cost function but the fastest convergence speed was when the mini batch size is 10, followed by when mini-batch size was 50 and 100 respectively.

While the gradient function, Beta_0, and Beta_1 all showed the same phenomenon as the cost function did.
```
## 4. Learning parametric models by second order optimisation (3 marks)

We now improve the optimisation by resorting to second order methods. One such method is Newton's method, which can be seen as adjusting the gradient to account for the local curvature in each step. The curvature is measured by the so-called Hessian of the cost function, which is the matrix of second order (partial) derivatives.

::: boxed-text
#### ðŸ’ª Problem 4.1

Derive (analytically) the Hessian of the log-likelihood $\ell(\beta_0, \beta_1)$ for the Poisson regression model.

::: callout-tip
Because of the (conditional) independence assumption $$\nabla\nabla^\top \ell(\beta_0, \beta_1)=\sum_{i=1}^n \nabla\nabla^\top \log p(y_i|x_i,\beta_0, \beta_1),$$ and note that

$$
\nabla\nabla^\top \log p(y_i|x_i,\beta_0, \beta_1) = \begin{bmatrix}
\frac{\partial^2}{\partial\beta_0^2}\log p(y_i|x_i,\beta_0, \beta_1) & \frac{\partial^2}{\partial\beta_0\partial\beta_1}\log p(y_i|x_i,\beta_0, \beta_1) \\
\frac{\partial^2}{\partial\beta_1\partial\beta_0}\log p(y_i|x_i,\beta_0, \beta_1) & \frac{\partial^2}{\partial\beta_1^2}\log p(y_i|x_i,\beta_0, \beta_1)
\end{bmatrix}.
$$
:::
:::

Newton's method is essentially derived using a second order Taylor expansion around the current parameter update. The next parameter update uses this expansion, which is known to be accurate only when the next parameter update is close to the current update. When not accurate, the updates tend to diverge, making Newton's method unstable. The trust region Newton's method is one way to overcome this instability by allowing the algorithm to take a Newton step only when the approximation is likely to be accurate (in the so-called trust region) or, otherwise, resort to a standard gradient descent step (without the Hessian).

It is a bit tricky to implement the sum of Hessians efficiently in R. Below follows a simple (and slow) implementation which is good enough for our purpose. It uses the following functions:

-   `Hess_log_dens_single_obs()`: This function returns the Hessian for a single observation evaluated at the vector `beta` ($\beta_0, \beta_1$). The argument `y_i` is the response of the $i$th observation, i.e. $y_i$, and `X_i` is the feature of $i$th observation (possibly including the intercept). In Problem 4.2 below you have to replace the `NA` values of $\phi_{kl}$ with the ones you derived in Problem 4.1, where $$\phi_{kl}=\frac{\partial^2}{\partial\beta_k\partial\beta_l}\log p(y_i|x_i,\beta_k, \beta_l).$$

-   `Hess_log_like()`: This function loops over the observations and sums their Hessians. The argument `y` contains the response of all observations and `X` contains the features of all observations (possibly including the intercept). When `Hess_log_dens_single_obs()` is implemented correctly, `Hess_log_like()` returns a $2\times 2$ matrix.

```{r}
Hess_log_dens_single_obs <- function(beta, y_i, X_i) {
  mu <- exp(sum(beta * X_i))
  phi11 <- -mu
  phi12 <- -mu * X_i[2]
  phi21 <- phi12
  phi22 <- -mu * X_i[2]^2
  return(matrix(c(phi11, phi21, phi12, phi22), nrow = 2, ncol = 2))
}

Hess_log_like <- function(beta, y, X) {
  n <- length(y)
  sum_Hess_log_like <- matrix(rep(0, 4), nrow = 2, ncol = 2)
  for(i in 1:n) {
    sum_Hess_log_like <- sum_Hess_log_like + Hess_log_dens_single_obs(beta, y[i], X[i, ])
  }
  return(sum_Hess_log_like)
}

```

::: boxed-text
#### ðŸ’ª Problem 4.2

Fill in the `NA` values in the function `Hess_log_dens_single_obs()` above.
:::

::: boxed-text
#### ðŸ’ª Problem 4.3

Implement a trust region Newton's method (Algorithm 5.2 in Lindholm et al. (2022)) to learn the parameters in the Poisson regression model. Run the algorithm for 200 iterations using starting values $(0,0)$ and trust region radius $D=1$. Compare the convergence of this method to that of the gradient descent method in Problem 3.6.

::: callout-tip
In addition to the two functions you coded in Problem 3.6, you need $$\nabla\nabla^\top J(\beta_0, \beta_1) = -\frac{1}{n}\nabla\nabla^\top\ell(\beta_0, \beta_1),$$which you can compute using `Hess_log_like()` above (after filling in the `NA` values in the function `Hess_log_dens_single_obs()`).
:::
:::

```{r}
# New definition of functions for X
gradient_log_likelihood <- function(beta, y, X) {
  mu <- exp(X %*% beta)
  grad_beta <- t(X) %*% (y - mu)
  return(as.vector(grad_beta))
}

cost_function <- function(beta, y, X) {
  mu <- exp(X %*% beta)
  log_likelihood_value <- sum(y * log(mu) - mu - lfactorial(y))
  return(-log_likelihood_value / length(y))
}

# Algorithm 5.2
trust_region_newton <- function(y, X, beta_init, radius, max_iter = 200) {
  beta_history <- matrix(0, nrow = max_iter, ncol = 2)
  cost_history <- numeric(max_iter)
  gradient_norm_history <- numeric(max_iter)
  
  beta <- beta_init
  
  for (i in 1:max_iter) {
    # Calculate gradient and Hessian
    grad <- gradient_log_likelihood(beta, y, X)
    H <- Hess_log_like(beta, y, X)
    
    gradient_norm_history[i] <- norm(grad, type = "2")
    
    step <- -solve(H, grad)
    
    # Trust region step adjustment
    if (norm(step, type = "2") > radius) {
      step <- (radius / norm(step, type = "2")) * step
    }
    
    beta <- beta + step
    
    beta_history[i, ] <- beta
    cost_history[i] <- cost_function(beta, y, X)
  }
  
  return(list(beta_history = beta_history, 
              cost_history = cost_history, 
              gradient_norm_history = gradient_norm_history))
}

# Example data
x <- gen_x()
y <- rpois(1000, lambda = exp(0.3 + 2.5 * x))

X <- cbind(1, x)
beta_init <- c(0, 0)

result_trust_region <- trust_region_newton(y, X, beta_init, radius = 1)

# Plot
par(mfrow = c(2, 2))

plot(result_trust_region$cost_history, type = "l", 
     main = "Trust Region Cost Function", 
     xlab = "Iteration", ylab = "Cost")

plot(result_trust_region$gradient_norm_history, type = "l",
     main = "Gradient Norm Over Iterations",
     xlab = "Iteration", ylab = "Gradient Norm")

plot(result_trust_region$beta_history[, 1], type = "l", 
     main = "Beta_0 Convergence", 
     xlab = "Iteration", ylab = "Beta_0")

plot(result_trust_region$beta_history[, 2], type = "l", 
     main = "Beta_1 Convergence", 
     xlab = "Iteration", ylab = "Beta_1")
```

```{=html}
Here, we can conclude that the trust region Newton's method converges. 
It converges much faster than the gradient descent method in Problem 3.6.
```
## 5. Learning parametric models using the optim function (4 marks)

We now turn to how to use powerful built-in functions to carry out optimisation. One such function is `'optim()'`. By default, this function performs minimisation, however, it is straightforward to adapt it for maximisation by multiplying the objective function by $-1$.

The main input to the `'optim()'` function is `'fn'`, which is the function to be optimised. We need to code this function ourselves, and the first argument to this function is what `'optim()'` optimises over (in our case it should be the vector $\beta$). If the function has further inputs, such as in our example when we also need to supply the data, these have to be supplied to the `'optim()'` function too (this is illustrated in the tip to Problem 5.2).

The following code implements the log-likelihood function for the Poisson regression model in two steps. First, the function `'log_dens_Poisson()'` computes the log-density for each of the $n=1000$ observations. Note that this function is vectorised, i.e. it does not use any for-loops, which is much faster. Note also that the function is not assuming a fixed length of the `'beta'` input, so if we have more regression coefficients (more than 1 feature), the code will still function properly. It only requires that the number of columns of `'X'` matches the length of `'beta'`, which is also a requirement in the formulation of the model (@eq-PoisReg), so that the dot product is defined. Second, the function `'log_like_Poisson()'` sums the $n=1000$ elements of the vector returned by `'log_dens_Poisson()'`.

```{r}
log_dens_Poisson <- function(beta, y, X) {
  # log-density for each y_i, X_i
  return(dpois(y, lambda = exp(X%*%beta),log = TRUE))
}

log_like_Poisson <- function(beta, y, X) {
  # log-likelihood (for all observations)
  return(sum(log_dens_Poisson(beta, y, X)))
}

```

::: boxed-text
#### ðŸ’ª Problem 5.1

Code a cost function $J(\beta)$ that uses the vectorised functions above (`'log_dens_Poisson()'` and `'log_like_Poisson()'`).

::: callout-tip
Although $J(\beta)$ is, in a mathematical sense, a function of $\beta$ only, you have to supply both `'y'` and `'X'` to the R function (because both `'log_dens_Poisson()'` and `'log_like_Poisson()'` require them).
:::
:::

```{r}
cost_function_poisson <- function(beta, y, X) {
  return(-log_like_Poisson(beta, y, X)) #log_dens_Poisson() implemented inside
}
```

::: boxed-text
#### ðŸ’ª Problem 5.2

Use the `'optim()'` function to minimise the objective function $J(\beta)$. Can you retrieve the true parameter values?

::: callout-tip
The `'optim()'` function has a variety of inputs and you are encouraged to use the `'help()'` function to learn more. This function will become indispensable for you in your future career, so familiarise yourself with it!

A standard way to call the function (assuming the extra arguments to `J` are `y` and `X`) is `'obj<-optim(beta_start, J, method="L-BFGS-B", y=y, X=X)'`, where `'beta_start'` is a vector of starting values, `'J'` is the function to minimise, and `'method'` is the method chosen.
:::
:::

```{r}
X <- cbind(1, gen_x())
y <- rpois(1000, lambda = exp(0.3 + 2.5 * X[, 2])) 

beta_start <- c(0, 0)

result <- optim(par = beta_start, fn = cost_function_poisson, 
                method="L-BFGS-B", y = y, X = X)
result$par
```

We now turn to estimating a Poisson regression for real data with more features. The dataset `'eBay_Coins.RData'` contains observations from 1000 eBay auctions of coins. The dataset can be downloaded from the Canvas page of the course. The response variable is `'nBids'`, which records the number of bids in each auction. The remaining variables are the following features (including the intercept):

-   `'Const'`: Intercept.

-   `'PowerSeller'`: Is the seller selling large volumes on eBay?

-   `'VerifyID'`: Is the seller verified by eBay?

-   `'Sealed'`: Was the coin sold sealed in a never opened envelope?

-   `'MinBlem'`: Did the coin have a minor defect?

-   `'MajBlem'`: Did the coin have a major defect?

-   `'LargNeg'`: Did the seller get a lot of negative feedback from customers?

-   `'LogBook'`: Logarithm of the coins book value according to expert sellers. Standardised.

-   `'MinBidShare'`: A variable that measures the ratio of the minimum selling price (starting price) to the book value. Standardised.

::: boxed-text
#### ðŸ’ª Problem 5.3

Use the `'optim()'` function to learn the parameters in the Poisson regression model for the eBay dataset. Note that you have to read in the dataset yourself this time.
:::

```{r}
load(file = '/Users/chenxinghe/Desktop/Hugo/ESILV/A5/S9_UTS/37401_ML/ComputerLab/2/eBay_coins.RData')

y <- eBay_coins$nBids  # Response variable
X <- as.matrix(eBay_coins[, c("Const", "PowerSeller", "VerifyID", "Sealed", 
                              "Minblem", "MajBlem", "LargNeg", 
                              "LogBook",   "MinBidShare")])

beta_start <- rep(0, ncol(X))
optim_result <- optim(par = beta_start, fn = cost_function_poisson, 
                      method = "L-BFGS-B", y = y, X = X)

optim_result$par
```

::: boxed-text
#### ðŸ’ª Problem 5.4

Suppose there is a new auction with features:

-   `'PowerSeller'=1`,

-   `'VerifyID'=1`,

-   `'Sealed'=1`,

-   `'MinBlem'=0`,

-   `'MajBlem'=0`,

-   `'LargNeg'=0`,

-   `'LogBook'=1`,

-   `'MinBidShare'=0.5`.

Provide a point estimate of the expected number of bidders for this auction.
:::

```{r}
beta_estimated <- optim_result$par

# Define the feature vector for the new auction
X_new <- c(1,
           1,
           1,
           1,
           0,
           0,
           0,
           1,
           0.5)

expected_bidders <- exp(sum(X_new * beta_estimated))
expected_bidders
```
